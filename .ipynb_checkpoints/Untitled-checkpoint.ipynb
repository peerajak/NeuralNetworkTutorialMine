{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "  \"\"\"\n",
    "  Evaluate a numeric gradient for a function that accepts a numpy\n",
    "  array and returns a numpy array.\n",
    "  \"\"\"\n",
    "  grad = np.zeros_like(x)\n",
    "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "  while not it.finished:\n",
    "    ix = it.multi_index\n",
    "    \n",
    "    oldval = x[ix]\n",
    "    x[ix] = oldval + h\n",
    "    pos = f(x)\n",
    "    x[ix] = oldval - h\n",
    "    neg = f(x)\n",
    "    x[ix] = oldval\n",
    "    \n",
    "    grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "    it.iternext()\n",
    "   \n",
    "  return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=False, h=0.00001):\n",
    "  \"\"\" \n",
    "  a naive implementation of numerical gradient of f at x \n",
    "  - f should be a function that takes a single argument\n",
    "  - x is the point (numpy array) to evaluate the gradient at\n",
    "  \"\"\" \n",
    "\n",
    "  fx = f(x) # evaluate function value at original point\n",
    "  grad = np.zeros_like(x)\n",
    "  # iterate over all indexes in x\n",
    "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "  while not it.finished:\n",
    "\n",
    "    # evaluate function at x+h\n",
    "    ix = it.multi_index\n",
    "    oldval = x[ix]\n",
    "    x[ix] = oldval + h # increment by h\n",
    "    fxph = f(x) # evalute f(x + h)\n",
    "    x[ix] = oldval - h\n",
    "    fxmh = f(x) # evaluate f(x - h)\n",
    "    x[ix] = oldval # restore\n",
    "\n",
    "    # compute the partial derivative with centered formula\n",
    "    grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "    if verbose:\n",
    "      print ix, grad[ix]\n",
    "    it.iternext() # step to next dimension\n",
    "\n",
    "  return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "  \"\"\"\n",
    "  Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "  The input x has shape (N, d_1, ..., d_k) where x[i] is the ith input.\n",
    "  We multiply this against a weight matrix of shape (D, M) where\n",
    "  D = \\prod_i d_i\n",
    "\n",
    "  Inputs:\n",
    "  x - Input data, of shape (N, d_1, ..., d_k)\n",
    "  w - Weights, of shape (D, M)\n",
    "  b - Biases, of shape (M,)\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - out: output, of shape (N, M)\n",
    "  - cache: (x, w, b)\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the affine forward pass. Store the result in out. You     #\n",
    "  # will need to reshape the input into rows.                                 #\n",
    "  #############################################################################\n",
    "  N = x.shape[0]\n",
    "  D = np.prod(x.shape[1:])\n",
    "  x2 = np.reshape(x, (N, D))\n",
    "  out = np.dot(x2, w) + b\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  cache = (x, w, b)\n",
    "  return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Computes the backward pass for an affine layer.\n",
    "\n",
    "  Inputs:\n",
    "  - dout: Upstream derivative, of shape (N, M)\n",
    "  - cache: Tuple of:\n",
    "    - x: Input data, of shape (N, d_1, ... d_k)\n",
    "    - w: Weights, of shape (D, M)\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "  - dw: Gradient with respect to w, of shape (D, M)\n",
    "  - db: Gradient with respect to b, of shape (M,)\n",
    "  \"\"\"\n",
    "  x, w, b = cache\n",
    "  dx, dw, db = None, None, None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the affine backward pass.                                 #\n",
    "  #############################################################################\n",
    "  N = x.shape[0]\n",
    "  D = x.shape[1]\n",
    " \n",
    "\n",
    "  dx = np.dot(dout, w.T) # N x D\n",
    "  dw = np.dot(x.T, dout) # D x M\n",
    "  db = np.dot(dout.T, np.ones(N)) # M x 1\n",
    "\n",
    "\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.53455128  0.39314168 -0.82414949]] [[-1.53455128  0.39314168 -0.82414949]]\n",
      "Testing affine_backward function:\n",
      "dx error:  6.48711769279e-12\n",
      "dw error:  1.10010080021e-10\n",
      "db error:  2.27556342422e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(1, 3)\n",
    "w = np.random.randn(3, 2)\n",
    "b = np.random.randn(2)\n",
    "dout = np.random.randn(1, 2)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "print dx_num, dx\n",
    "# The error should be less than 1e-10\n",
    "print 'Testing affine_backward function:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dw error: ', rel_error(dw_num, dw)\n",
    "print 'db error: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_forward(z):\n",
    "        sigmoidfn = 1.0 / (1.0 + np.exp(-z)) # sigmoid activation function\n",
    "        return sigmoidfn\n",
    "    \n",
    "def sigmoid_backward(dout,fw_activation):\n",
    "        return np.multiply(np.multiply(fw_activation,1-fw_activation),dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05808236  0.03889595 -0.20467111]] [[-0.05808236  0.03889595 -0.20467111]]\n",
      "Testing affine_backward function:\n",
      "dx error:  5.52998852762e-11\n",
      "dw error:  5.96447051052e-11\n",
      "db error:  1.59729233003e-11\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(1, 3)\n",
    "w = np.random.randn(3, 2)\n",
    "b = np.random.randn(2)\n",
    "dout = np.random.randn(1, 2)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: sigmoid_forward(affine_forward(x, w, b)[0]), x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: sigmoid_forward(affine_forward(x, w, b)[0]), w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: sigmoid_forward(affine_forward(x, w, b)[0]), b, dout)\n",
    "\n",
    "y, cache = affine_forward(x, w, b)\n",
    "sigout = sigmoid_forward(y)\n",
    "douty = sigmoid_backward(dout,sigout)\n",
    "dx, dw, db = affine_backward(douty, cache)\n",
    "\n",
    "print dx_num, dx\n",
    "# The error should be less than 1e-10\n",
    "print 'Testing affine_backward function:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dw error: ', rel_error(dw_num, dw)\n",
    "print 'db error: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_sq_loss(z,t):\n",
    "    #z is lower layer's activation, t is the target value\n",
    "    m = z.shape[0] #number of training sample in a batch\n",
    "    n = z.shape[1] #size of feature\n",
    "\n",
    "    loss = 0.5* np.dot( z[0,:]-t[0,:],np.transpose(z[0,:]-t[0,:]) )\n",
    "    dout = (z - t)\n",
    "    \n",
    "    return loss,dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piano_fw(x,w,b,t):\n",
    "    loss,_ = euclidean_sq_loss(sigmoid_forward(affine_forward(x, w, b)[0]),t)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47809586764 0.47809586764\n",
      "dw_num: [[-0.00084517  0.00261561]\n",
      " [ 0.00769501 -0.02381437]\n",
      " [ 0.00949257 -0.02937739]] db_num: [ 0.00820025 -0.02537797]\n",
      "[[-0.00084517  0.00261561]\n",
      " [ 0.00769501 -0.02381437]\n",
      " [ 0.00949257 -0.02937739]] [ 0.00820025 -0.02537797]\n",
      "dw error:  7.43419912547e-10\n",
      "db error:  8.05032862384e-11\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(1, 3)\n",
    "w = np.random.randn(3, 2)\n",
    "b = np.random.randn(2)\n",
    "t = np.array([[0,1]])\n",
    "\n",
    "y, cache = affine_forward(x, w, b)\n",
    "sigout = sigmoid_forward(y)\n",
    "loss,dout = euclidean_sq_loss(sigout,t)\n",
    "print loss, 0.5* (np.linalg.norm(sigout-t))**2\n",
    "\n",
    "dw_num = eval_numerical_gradient(lambda w: piano_fw(x,w,b,t), w)\n",
    "db_num = eval_numerical_gradient(lambda b: piano_fw(x,w,b,t), b)\n",
    "\n",
    "print 'dw_num:',dw_num,'db_num:',db_num\n",
    "\n",
    "douty = sigmoid_backward(dout,sigout)\n",
    "dx, dw, db = affine_backward(douty, cache)\n",
    "print dw,db\n",
    "#print dx_num, dx\n",
    "# The error should be less than 1e-10\n",
    "#print 'Testing affine_backward function:'\n",
    "#print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dw error: ', rel_error(dw_num, dw)\n",
    "print 'db error: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet_Piano:\n",
    "    #class variables\n",
    "    # num_classes  is the size of the last affine activation\n",
    "    # input_dim   is the size of input dimension\n",
    "    # num_hidden_layers is how many hidden layers\n",
    "    ## (constructor ) Dict model\n",
    "    ## (constructor) num_layers is how many totol layers = hidden_layers + 2(Softmax cross entropy loss layer)\n",
    "    ## (Forward) Dict input cache\n",
    "    ## (Backward) Dict gradient cache \n",
    "    #class def\n",
    "    # constructor to construct the neural network, given the hyper parameter\n",
    "    # Forward\n",
    "    # Backward\n",
    "    # applyGrad\n",
    "    def __init__(self,model_hyper):\n",
    "        num_classes = model_hyper['num_classes']\n",
    "        input_dim  = model_hyper['input_dim']\n",
    "        self.num_hidden_layers = model_hyper['num_hidden_layers']\n",
    "        self.num_layers = self.num_hidden_layers + 1\n",
    "        layer_list = [None] * self.num_hidden_layers\n",
    "        cur_layer_size = input_dim\n",
    "        self.learnrate = model_hyper['learnrate']\n",
    "        self.NN_model = []\n",
    "        self.NN_grad = []\n",
    "        self.NN_num_grad = []\n",
    "        for hid_layer_iter in range(self.num_hidden_layers):  \n",
    "            layer_model = {}\n",
    "            next_layer_size = model_hyper['layer_sizes'][hid_layer_iter]\n",
    "            layer_model['W'] = 0.9 * np.random.randn(cur_layer_size, next_layer_size)\n",
    "            layer_model['b'] =0.9 * np.random.randn(next_layer_size)\n",
    "            self.NN_model.append(layer_model)\n",
    "            cur_layer_size = next_layer_size\n",
    "        \n",
    "        layer_model = {}   \n",
    "        next_layer_size = num_classes\n",
    "        layer_model['W'] = 0.9 * np.random.randn(cur_layer_size, next_layer_size)\n",
    "        self.NN_model.append(layer_model)\n",
    "        self.NN_cache=[]\n",
    "        self.sigmoid_on = False\n",
    "        for i in self.NN_model:\n",
    "            print i['W'].shape\n",
    "            if 'b' in i:\n",
    "                print i['b'].shape\n",
    "            print \"__________________\"\n",
    "            \n",
    "    def affine_backward(self,dout, cache):\n",
    "        x = cache['x']\n",
    "        w = cache['w']\n",
    "        b = cache['b']\n",
    "        #print x,w,b\n",
    "        grad_cache = {}\n",
    "        dx, dw, db = None, None, None\n",
    "        N = x.shape[0]\n",
    "        D = np.prod(x.shape[1:])\n",
    "        x2 = np.reshape(x, (N, D))\n",
    "\n",
    "        dx2 = np.dot(dout, w.T) # N x D\n",
    "        dw = np.dot(x2.T, dout) # D x M\n",
    "        db = np.dot(dout.T, np.ones(N)) # M x 1\n",
    "\n",
    "        dx = np.reshape(dx2, x.shape)\n",
    "        grad_cache['dx'] = dx\n",
    "        grad_cache['dw'] = dw\n",
    "        #print grad_cache['dw']\n",
    "        grad_cache['db'] = db\n",
    "        return grad_cache\n",
    "    \n",
    "    def affine_backward2(self,dout, cache):\n",
    "        x = cache['x']\n",
    "        w = cache['w']\n",
    "\n",
    "        grad_cache = {}\n",
    "        dx, dw = None, None\n",
    "        N = x.shape[0]\n",
    "        D = np.prod(x.shape[1:])\n",
    "        x2 = np.reshape(x, (N, D))\n",
    "\n",
    "        dx2 = np.dot(dout, w.T) # N x D\n",
    "        dw = np.dot(x2.T, dout) # D x M       \n",
    "\n",
    "        dx = np.reshape(dx2, x.shape)\n",
    "        grad_cache['dx'] = dx\n",
    "        grad_cache['dw'] = dw\n",
    " \n",
    "        return grad_cache  \n",
    "\n",
    "    def affine_forward(self,x, w, b): \n",
    "        cache = {}\n",
    "        out = None\n",
    "        N = x.shape[0]\n",
    "        D = np.prod(x.shape[1:])\n",
    "        x2 = np.reshape(x, (N, D))\n",
    "        out = np.dot(x2, w) + b\n",
    "        cache['x'] = x\n",
    "        cache['w'] = w\n",
    "        cache['b'] = b\n",
    "        return out, cache    \n",
    "    \n",
    "    def affine_forward2(self,x, w): \n",
    "        out = None\n",
    "        cache = {}\n",
    "        N = x.shape[0]\n",
    "        D = np.prod(x.shape[1:])\n",
    "        x2 = np.reshape(x, (N, D))\n",
    "        out = np.dot(x2, w) \n",
    "        cache['x'] = x\n",
    "        cache['w'] = w\n",
    "        return out, cache  \n",
    "\n",
    "    def sigmoid_forward(self,z):\n",
    "        sigmoidfn = 1.0 / (1.0 + np.exp(-z)) # sigmoid activation function\n",
    "        return sigmoidfn\n",
    "    \n",
    "    def sigmoid_backward(self,dout,fw_activation):\n",
    "        return np.multiply(np.multiply(fw_activation,1-fw_activation),dout)\n",
    "\n",
    "    def euclidean_sq_loss(self,z,t):\n",
    "        #z is lower layer's activation, t is the target value\n",
    "        m = z.shape[0] #number of training sample in a batch\n",
    "        n = z.shape[1] #size of feature\n",
    "        #print z.shape, t.shape\n",
    "        loss = 0.5* np.dot( z[0,:]-t[0,:],np.transpose(z[0,:]-t[0,:]) )\n",
    "        dout = (z - t)\n",
    "    \n",
    "        return loss,dout\n",
    "    \n",
    "    def forward_train(self,x,y):\n",
    "        activation = x\n",
    "        #print activation\n",
    "\n",
    "        for i in self.NN_model:\n",
    "            if 'b' in i:\n",
    "                activation, cache = self.affine_forward(activation, i['W'], i['b'])\n",
    "                self.NN_cache.append(cache)\n",
    "            else:\n",
    "                activation, cache = self.affine_forward2(activation, i['W'])\n",
    "                self.NN_cache.append(cache)\n",
    "        \n",
    "\n",
    "        if (self.sigmoid_on == True):\n",
    "            sigout = self.sigmoid_forward(activation) \n",
    "            self.NN_cache.append(sigout)\n",
    "            loss,dz = self.euclidean_sq_loss(sigout,y)\n",
    "        else:\n",
    "            loss,dz = self.euclidean_sq_loss(activation,y)\n",
    "        #for i in self.NN_cache:\n",
    "        #    print i['w'].shape,i['x'].shape\n",
    "        #    if 'b' in i:\n",
    "        #        print i['b'].shape\n",
    "        #    print \"__________________\"   \n",
    "        return loss,dz\n",
    "    \n",
    "    def forward_test(self,x):\n",
    "        activation = x\n",
    "        #print activation\n",
    "\n",
    "        for i in self.NN_model:\n",
    "            if 'b' in i:\n",
    "                activation, cache = self.affine_forward(activation, i['W'], i['b'])\n",
    "                self.NN_cache.append(cache)\n",
    "            else:\n",
    "                activation, cache = self.affine_forward2(activation, i['W'])\n",
    "                self.NN_cache.append(cache)\n",
    " \n",
    "        return self.sigmoid_forward(activation)\n",
    "    \n",
    "    def backward_train(self,dz):\n",
    "        if self.sigmoid_on == True:\n",
    "            sigactivation = self.NN_cache[-1]\n",
    "            backgrad = sigmoid_backward(dz,sigactivation)\n",
    "            for i in reversed(self.NN_cache[:-1]):\n",
    "                if 'b' in i:\n",
    "                    grad_cache = self.affine_backward(backgrad,i)\n",
    "                else:\n",
    "                    grad_cache = self.affine_backward2(backgrad,i)\n",
    "                backgrad = grad_cache['dx']\n",
    "                \n",
    "            self.NN_grad.insert(0,grad_cache)\n",
    "        else:\n",
    "            backgrad = dz\n",
    "            for i in reversed(self.NN_cache):\n",
    "                if 'b' in i:\n",
    "                    grad_cache = self.affine_backward(backgrad,i)\n",
    "                else:\n",
    "                    grad_cache = self.affine_backward2(backgrad,i)\n",
    "                backgrad = grad_cache['dx']\n",
    "                \n",
    "            self.NN_grad.insert(0,grad_cache)     \n",
    "\n",
    "            #self.NN_grad.append(grad_cache)\n",
    "    def Apply_UpdateW(self):\n",
    "        for i in range(self.num_layers):\n",
    "            delta_W = -self.learnrate * self.NN_grad[i]['dw'] \n",
    "            print delta_W\n",
    "            self.NN_model[i]['W'] += delta_W\n",
    "            print 'Apply Update',i\n",
    "            if 'db' in self.NN_grad[i] :\n",
    "                delta_b = -self.learnrate * self.NN_grad[i]['db'] \n",
    "                self.NN_model[i]['b'] += delta_b\n",
    "        self.reset_cache()\n",
    "\n",
    "    def reset_cache(self):\n",
    "        del(self.NN_cache[:])\n",
    "        del(self.NN_grad[:]) \n",
    "        del(self.NN_num_grad[:]) \n",
    "        \n",
    "    def get_grad(self):\n",
    "        return self.NN_grad\n",
    "\n",
    "    def get_W(self):\n",
    "        return self.NN_model\n",
    "    def cal_num_grad(self,w,h,x,y):\n",
    "        grad = np.zeros_like(w)\n",
    "        #print \"w.shape\",w.shape\n",
    "        # iterate over all indexes in w\n",
    "        it = np.nditer(w, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        \n",
    "        while not it.finished:\n",
    "            \n",
    "            # evaluate function at x+h\n",
    "            ix = it.multi_index\n",
    "            oldval = w[ix]\n",
    "            w[ix] = oldval + h # increment by h\n",
    "            fxph,_ = self.forward_train(x,y) # evalute f(x + h)\n",
    "            w[ix] = oldval - h\n",
    "            fxmh,_ = self.forward_train(x,y) # evaluate f(x - h)\n",
    "            w[ix] = oldval # restore\n",
    "            # compute the partial derivative with centered formula\n",
    "            grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "            it.iternext()           \n",
    "        return grad\n",
    "    def get_numerical_gradient(self):\n",
    "        return self.NN_num_grad\n",
    "    def set_numerical_gradient(self,x,y, h=0.0001):        \n",
    "        for i in self.NN_model:\n",
    "            num_grad_cache = {}\n",
    "            num_grad_cache['dw'] = self.cal_num_grad(i['W'],h,x,y)\n",
    "            \n",
    "            if 'b' in i:\n",
    "                num_grad_cache['db'] = self.cal_num_grad(i['b'],h,x,y)\n",
    "               \n",
    "            self.NN_num_grad.append(num_grad_cache)\n",
    "            \n",
    "        return self.NN_num_grad\n",
    "    \n",
    "    def check_grad_numgrad(self,x,y, h=0.0001,verbose = True):       \n",
    "        self.set_numerical_gradient(x,y, h)  \n",
    "        if verbose:\n",
    "            for i in range(self.num_hidden_layers+1):\n",
    "                print i,\"th layer\"\n",
    "                if (rel_error(self.NN_grad[i]['dw'],self.NN_num_grad[i]['dw'])<1e-7):\n",
    "                    print 'grad W correct',rel_error(self.NN_grad[i]['dw'],self.NN_num_grad[i]['dw'])\n",
    "                else:\n",
    "                    print self.NN_grad[i]['dw'].flatten()\n",
    "                    print self.NN_num_grad[i]['dw'].flatten()\n",
    "                    print rel_error(self.NN_grad[i]['dw'],self.NN_num_grad[i]['dw'])\n",
    "                    print np.linalg.norm(self.NN_grad[i]['dw'].flatten() - self.NN_num_grad[i]['dw'].flatten())**2\n",
    "                    print '==========='\n",
    "                if 'db' in self.NN_grad[i] :\n",
    "                    if (rel_error(self.NN_grad[i]['db'],self.NN_num_grad[i]['db'])<1e-7):\n",
    "                        print 'grad b correct',rel_error(self.NN_grad[i]['db'],self.NN_num_grad[i]['db'])\n",
    "                    else:\n",
    "                        print self.NN_grad[i]['db'].flatten()\n",
    "                        print self.NN_num_grad[i]['db'].flatten()\n",
    "                        print rel_error(self.NN_grad[i]['db'],self.NN_num_grad[i]['db'])\n",
    "                        print '==========='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hyper = {}\n",
    "model_hyper['num_classes'] = 4\n",
    "model_hyper['input_dim']=4\n",
    "model_hyper['num_hidden_layers']=0\n",
    "model_hyper['learnrate']=0.0001\n",
    "layer_size = [None] * model_hyper['num_hidden_layers']\n",
    "#layer_size[0] = 4\n",
    "#layer_size[1] = 4\n",
    "#layer_size[2] = 5\n",
    "#layer_size[3] = 6\n",
    "model_hyper['layer_sizes'] = layer_size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "__________________\n",
      "xshape (1, 4) (1, 4)\n",
      "[[0 0 1 0]] \n",
      "[[ 0.  0.  1.  0.]]\n",
      "----------\n",
      "dz shape (1, 4)\n",
      "0 th layer\n",
      "grad W correct 1.49319282588e-11\n",
      "int64 float64\n"
     ]
    }
   ],
   "source": [
    "a_nn = NeuralNet_Piano(model_hyper)\n",
    "w_org = a_nn.get_W()\n",
    "w_org0 = w_org[0]['W'].copy()\n",
    "num_inputs = 1\n",
    "x = np.random.randn(num_inputs, model_hyper['input_dim'])\n",
    "y = np.random.randint(model_hyper['num_classes'] , size=num_inputs)\n",
    "y_onehot=np.zeros([num_inputs,model_hyper['num_classes']])\n",
    "y_onehot[np.arange(num_inputs), y] = 1\n",
    "y = y_onehot.astype(int)\n",
    "print 'xshape',x.shape,y.shape\n",
    "print y,'\\n',y_onehot\n",
    "print '----------'\n",
    "loss,dz = a_nn.forward_train(x,y)\n",
    "print 'dz shape',dz.shape\n",
    "a_nn.backward_train(dz)\n",
    "grad = a_nn.get_grad()\n",
    "a_nn.check_grad_numgrad(x,y)\n",
    "print y.dtype,x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[[  3.00970625e-05  -5.27228171e-04   4.78362477e-04  -6.54646927e-04]\n",
      " [  3.01015855e-05  -5.27307403e-04   4.78434365e-04  -6.54745308e-04]\n",
      " [ -4.42938892e-05   7.75922440e-04  -7.04006729e-04   9.63444802e-04]\n",
      " [  4.55283656e-05  -7.97547499e-04   7.23627488e-04  -9.90296134e-04]]\n"
     ]
    }
   ],
   "source": [
    "w_after = a_nn.get_W()\n",
    "print w_org[0]['W'] - w_after[0]['W']\n",
    "grad_w = a_nn.get_grad()\n",
    "delta_w = -model_hyper['learnrate'] * grad_w [0]['dw'] \n",
    "print delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_grad = a_nn.get_numerical_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "(4, 4)\n",
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "for i in a_nn.get_W():\n",
    "    print i['W'].shape\n",
    "for i in grad:\n",
    "    print i['dw'].shape\n",
    "for i in num_grad:\n",
    "    print i['dw'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====num grad =====\n",
      "0 th layer:W error 1.49319282588e-11\n",
      "__________________\n",
      "[-0.30097063  5.27228171 -4.78362477  6.54646927 -0.30101586  5.27307403\n",
      " -4.78434365  6.54745308  0.44293889 -7.7592244   7.04006729 -9.63444802\n",
      " -0.45528366  7.97547499 -7.23627488  9.90296134]\n",
      "[-0.30097063  5.27228171 -4.78362477  6.54646927 -0.30101586  5.27307403\n",
      " -4.78434365  6.54745308  0.44293889 -7.7592244   7.04006729 -9.63444802\n",
      " -0.45528366  7.97547499 -7.23627488  9.90296134]\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "print '=====num grad ====='\n",
    "for i in range(model_hyper['num_hidden_layers']+1):\n",
    "    print i,\"th layer:W error\",rel_error(grad[i]['dw'],num_grad[i]['dw'])    \n",
    "    if 'db' in grad[i] :\n",
    "        print i,\"th layer:b error\",rel_error(grad[i]['db'],num_grad[i]['db'])\n",
    "    print \"__________________\"  \n",
    "\n",
    "print grad[0]['dw'].flatten()\n",
    "print num_grad[0]['dw'].flatten()\n",
    "print \"__________________\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.00970625e-05  -5.27228171e-04   4.78362477e-04  -6.54646927e-04]\n",
      " [  3.01015855e-05  -5.27307403e-04   4.78434365e-04  -6.54745308e-04]\n",
      " [ -4.42938892e-05   7.75922440e-04  -7.04006729e-04   9.63444802e-04]\n",
      " [  4.55283656e-05  -7.97547499e-04   7.23627488e-04  -9.90296134e-04]]\n",
      "Apply Update 0\n",
      "int64 float64\n",
      "dz shape (1, 4)\n",
      "0 th layer\n",
      "grad W correct 2.3823248661e-11\n",
      "=====num grad =====\n",
      "0 th layer:W error 2.3823248661e-11\n",
      "__________________\n",
      "[-0.30056661  5.26520431 -4.77720333  6.53768142 -0.30061178  5.26599557\n",
      " -4.77792125  6.53866391  0.4423443  -7.74880857  7.03061685 -9.62151493\n",
      " -0.45467249  7.96476887 -7.22656105  9.8896678 ]\n",
      "[-0.30056661  5.26520431 -4.77720333  6.53768142 -0.30061178  5.26599557\n",
      " -4.77792125  6.53866391  0.4423443  -7.74880857  7.03061685 -9.62151493\n",
      " -0.45467249  7.96476887 -7.22656105  9.8896678 ]\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "a_nn.Apply_UpdateW()\n",
    "print y.dtype,x.dtype\n",
    "loss,dz = a_nn.forward_train(x,y)\n",
    "print 'dz shape',dz.shape\n",
    "a_nn.backward_train(dz)\n",
    "grad = a_nn.get_grad()\n",
    "a_nn.check_grad_numgrad(x,y_onehot)\n",
    "num_grad = a_nn.get_numerical_gradient()\n",
    "print '=====num grad ====='\n",
    "for i in range(model_hyper['num_hidden_layers']+1):\n",
    "    print i,\"th layer:W error\",rel_error(grad[i]['dw'],num_grad[i]['dw'])    \n",
    "    if 'db' in grad[i] :\n",
    "        print i,\"th layer:b error\",rel_error(grad[i]['db'],num_grad[i]['db'])\n",
    "    print \"__________________\"  \n",
    "\n",
    "print grad[0]['dw'].flatten()\n",
    "print num_grad[0]['dw'].flatten()\n",
    "print \"__________________\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_after0:\n",
      "[[ 0.38626467  0.37073964 -0.16543359  1.0017585 ]\n",
      " [ 1.66452958  0.43935213 -1.69210078  0.59696873]\n",
      " [ 0.07950536  0.03226324 -0.18581565 -0.45950859]\n",
      " [-1.37405442  1.16940226 -0.01273694  0.57407696]]\n",
      "w_org0:\n",
      "[[ 0.38623457  0.37126687 -0.16591196  1.00241315]\n",
      " [ 1.66449947  0.43987943 -1.69257921  0.59762347]\n",
      " [ 0.07954966  0.03148731 -0.18511165 -0.46047203]\n",
      " [-1.37409995  1.17019981 -0.01346057  0.57506726]]\n",
      "real_delta_w:\n",
      "[[ -3.00970625e-05   5.27228171e-04  -4.78362477e-04   6.54646927e-04]\n",
      " [ -3.01015855e-05   5.27307403e-04  -4.78434365e-04   6.54745308e-04]\n",
      " [  4.42938892e-05  -7.75922440e-04   7.04006729e-04  -9.63444802e-04]\n",
      " [ -4.55283656e-05   7.97547499e-04  -7.23627488e-04   9.90296134e-04]]\n",
      "delta_w:\n",
      "[[ -3.00566608e-05   5.26520431e-04  -4.77720333e-04   6.53768142e-04]\n",
      " [ -3.00611778e-05   5.26599557e-04  -4.77792125e-04   6.53866391e-04]\n",
      " [  4.42344300e-05  -7.74880857e-04   7.03061685e-04  -9.62151493e-04]\n",
      " [ -4.54672493e-05   7.96476887e-04  -7.22656105e-04   9.88966780e-04]]\n"
     ]
    }
   ],
   "source": [
    "w_after = a_nn.get_W()\n",
    "w_after0= w_after[0]['W']\n",
    "real_delta_w = w_org0 - w_after0\n",
    "\n",
    "print 'w_after0:\\n',w_after0\n",
    "print 'w_org0:\\n',w_org0\n",
    "print 'real_delta_w:\\n',real_delta_w\n",
    "grad_w = a_nn.get_grad()\n",
    "delta_w = model_hyper['learnrate'] * grad_w [0]['dw'] \n",
    "print 'delta_w:\\n',delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.00566608e-05  -5.26520431e-04   4.77720333e-04  -6.53768142e-04]\n",
      " [  3.00611778e-05  -5.26599557e-04   4.77792125e-04  -6.53866391e-04]\n",
      " [ -4.42344300e-05   7.74880857e-04  -7.03061685e-04   9.62151493e-04]\n",
      " [  4.54672493e-05  -7.96476887e-04   7.22656105e-04  -9.88966780e-04]]\n",
      "Apply Update 0\n",
      "int64 float64\n",
      "dz shape (1, 4)\n",
      "0 th layer\n",
      "grad W correct 7.07169841035e-12\n",
      "=====num grad =====\n",
      "0 th layer:W error 7.07169841035e-12\n",
      "__________________\n",
      "[-0.30016313  5.2581364  -4.77079051  6.52890537 -0.30020824  5.2589266\n",
      " -4.77150747  6.52988654  0.44175051 -7.73840673  7.02117909 -9.60859921\n",
      " -0.45406215  7.95407713 -7.21686027  9.87639212]\n",
      "[-0.30016313  5.2581364  -4.77079051  6.52890537 -0.30020824  5.2589266\n",
      " -4.77150747  6.52988654  0.44175051 -7.73840673  7.02117909 -9.60859921\n",
      " -0.45406215  7.95407713 -7.21686027  9.87639212]\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "a_nn.Apply_UpdateW()\n",
    "print y.dtype,x.dtype\n",
    "loss,dz = a_nn.forward_train(x,y)\n",
    "print 'dz shape',dz.shape\n",
    "a_nn.backward_train(dz)\n",
    "grad = a_nn.get_grad()\n",
    "a_nn.check_grad_numgrad(x,y_onehot)\n",
    "num_grad = a_nn.get_numerical_gradient()\n",
    "print '=====num grad ====='\n",
    "for i in range(model_hyper['num_hidden_layers']+1):\n",
    "    print i,\"th layer:W error\",rel_error(grad[i]['dw'],num_grad[i]['dw'])    \n",
    "    if 'db' in grad[i] :\n",
    "        print i,\"th layer:b error\",rel_error(grad[i]['db'],num_grad[i]['db'])\n",
    "    print \"__________________\"  \n",
    "\n",
    "print grad[0]['dw'].flatten()\n",
    "print num_grad[0]['dw'].flatten()\n",
    "print \"__________________\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_after0:\n",
      "[[ 0.38629473  0.37021312 -0.16495587  1.00110473]\n",
      " [ 1.66455964  0.43882553 -1.69162299  0.59631486]\n",
      " [ 0.07946113  0.03303812 -0.18651871 -0.45854644]\n",
      " [-1.37400896  1.16860578 -0.01201429  0.57308799]]\n",
      "w_org0:\n",
      "[[ 0.38623457  0.37126687 -0.16591196  1.00241315]\n",
      " [ 1.66449947  0.43987943 -1.69257921  0.59762347]\n",
      " [ 0.07954966  0.03148731 -0.18511165 -0.46047203]\n",
      " [-1.37409995  1.17019981 -0.01346057  0.57506726]]\n",
      "real_delta_w:\n",
      "[[ -6.01537233e-05   1.05374860e-03  -9.56082809e-04   1.30841507e-03]\n",
      " [ -6.01627633e-05   1.05390696e-03  -9.56226491e-04   1.30861170e-03]\n",
      " [  8.85283191e-05  -1.55080330e-03   1.40706841e-03  -1.92559630e-03]\n",
      " [ -9.09956149e-05   1.59402439e-03  -1.44628359e-03   1.97926291e-03]]\n",
      "delta_w:\n",
      "[[ -3.00163134e-05   5.25813640e-04  -4.77079051e-04   6.52890537e-04]\n",
      " [ -3.00208243e-05   5.25892660e-04  -4.77150747e-04   6.52988654e-04]\n",
      " [  4.41750506e-05  -7.73840673e-04   7.02117909e-04  -9.60859921e-04]\n",
      " [ -4.54062150e-05   7.95407713e-04  -7.21686027e-04   9.87639212e-04]]\n"
     ]
    }
   ],
   "source": [
    "w_after = a_nn.get_W()\n",
    "w_after0= w_after[0]['W']\n",
    "real_delta_w = w_org0 - w_after0\n",
    "\n",
    "print 'w_after0:\\n',w_after0\n",
    "print 'w_org0:\\n',w_org0\n",
    "print 'real_delta_w:\\n',real_delta_w\n",
    "grad_w = a_nn.get_grad()\n",
    "delta_w = model_hyper['learnrate'] * grad_w [0]['dw'] \n",
    "print 'delta_w:\\n',delta_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_train = 50000\n",
    "y = np.random.randint(model_hyper['num_classes'] , size=num_train)\n",
    "y_onehot=np.zeros([num_train,model_hyper['num_classes']])\n",
    "\n",
    "y_onehot[np.arange(num_train), y] = 1\n",
    "x = y_onehot\n",
    "y = y_onehot.astype(int)\n",
    "model2_hyper = {}\n",
    "model2_hyper['num_classes'] = 4\n",
    "model2_hyper['input_dim']=4\n",
    "model2_hyper['num_hidden_layers']=1\n",
    "model2_hyper['learnrate']=0.001\n",
    "layer_size = [None] * model2_hyper['num_hidden_layers']\n",
    "layer_size[0] = 4\n",
    "#layer_size[1] = 4\n",
    "#layer_size[2] = 5\n",
    "#layer_size[3] = 6\n",
    "model2_hyper['layer_sizes'] = layer_size  \n",
    "\n",
    "batch_size =1\n",
    "num_train_batch = num_train/batch_size\n",
    "num_rec = num_train\n",
    "cost_rec = np.zeros(num_rec,dtype=np.float)\n",
    "a_nn.reset_cache()\n",
    "for ii in range(10):\n",
    "    for i in range(num_rec):\n",
    "        th_batch = i%num_train_batch\n",
    "        Xbatch = x[th_batch*batch_size:th_batch*batch_size+batch_size,:]\n",
    "        Ybatch = y[th_batch*batch_size:th_batch*batch_size+batch_size].astype(int)  \n",
    "        #print Xbatch\n",
    "        loss,dz = a_nn.forward_train(Xbatch,Ybatch)\n",
    "        print loss\n",
    "        a_nn.backward_train(dz)\n",
    "        #iris_nn.check_grad_numgrad(Xbatch,Ybatch,0.0001,True)\n",
    "        a_nn.Apply_UpdateW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'W': array([[ 0.38629473,  0.37021312, -0.16495587,  1.00110473],\n",
      "       [ 1.66455964,  0.43882553, -1.69162299,  0.59631486],\n",
      "       [ 0.07946113,  0.03303812, -0.18651871, -0.45854644],\n",
      "       [-1.37400896,  1.16860578, -0.01201429,  0.57308799]])}]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "Wmodel = a_nn.get_W()\n",
    "print Wmodel\n",
    "print len(Wmodel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 5)\n",
      "[[ 5.   3.5  1.6  0.6  2. ]\n",
      " [ 5.   3.6  1.4  0.2  2. ]\n",
      " [ 6.7  3.1  5.6  2.4  0. ]\n",
      " [ 7.3  2.9  6.3  1.8  0. ]]\n",
      "(4, 5)\n",
      "(5,)\n",
      "__________________\n",
      "(5, 4)\n",
      "(4,)\n",
      "__________________\n",
      "(4, 5)\n",
      "(5,)\n",
      "__________________\n",
      "(5, 3)\n",
      "__________________\n",
      "355.101208438\n",
      "[[ -1.25059127e-04  -1.90090355e-05   3.49577867e-04   2.61091103e-04\n",
      "   -1.18717917e-04]\n",
      " [ -2.29165940e-06  -3.48333112e-07   6.40587714e-06   4.78439194e-06\n",
      "   -2.17545922e-06]\n",
      " [  1.53213800e-04   2.32885566e-05  -4.28278643e-04  -3.19870775e-04\n",
      "    1.45444988e-04]\n",
      " [  2.35058779e-04   3.57290249e-05  -6.57059970e-04  -4.90741916e-04\n",
      "    2.23139960e-04]]\n",
      "Apply Update 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6972a408a8fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0miris_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdz_iris\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m#iris_nn.check_grad_numgrad(Xbatch,Ybatch,0.0001,True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0miris_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApply_UpdateW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-d1dc6269cf46>\u001b[0m in \u001b[0;36mApply_UpdateW\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mApply_UpdateW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mdelta_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearnrate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNN_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mdelta_W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNN_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdelta_W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "DataXy_ = np.loadtxt(open(\"iris_prep.txt\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "print DataXy_.shape\n",
    "rand_idx = np.random.permutation(149)\n",
    "DataXy =  DataXy_[rand_idx,:]\n",
    "print DataXy[0:4,:]\n",
    "num_train = 100\n",
    "num_test = 49\n",
    "trainX = DataXy[0:num_train,:-1]\n",
    "meanX = trainX.mean()\n",
    "#print trainX\n",
    "trainX = 0.1*(trainX-meanX) \n",
    "trainY = DataXy[0:num_train,-1]\n",
    "trainY_onehot = np.zeros([num_train,3])\n",
    "trainY_onehot[np.arange(num_train), trainY.astype(int)] = 1\n",
    "trainY = trainY_onehot\n",
    "testX = 0.1*(DataXy[num_train:,:-1]-meanX) \n",
    "testY = DataXy[num_train:,-1]\n",
    "testY_onehot = np.zeros([num_test,3])\n",
    "testY_onehot[np.arange(num_test), testY.astype(int)] = 1\n",
    "testY = testY_onehot\n",
    "#print trainX\n",
    "\n",
    "model_hyper2 = {}\n",
    "model_hyper2['num_classes'] = 3\n",
    "model_hyper2['input_dim']=4\n",
    "model_hyper2['num_hidden_layers']=3\n",
    "model_hyper2['learnrate']=0.00001\n",
    "layer_size = [None] * model_hyper2['num_hidden_layers']\n",
    "layer_size[0] = 5\n",
    "layer_size[1] = 4\n",
    "layer_size[2] = 5\n",
    "#layer_size[3] = 6\n",
    "model_hyper2['layer_sizes'] = layer_size    \n",
    "\n",
    "batch_size =1\n",
    "num_train_batch = num_train/batch_size\n",
    "num_rec = num_train\n",
    "cost_rec = np.zeros(num_rec,dtype=np.float)\n",
    "iris_nn = NeuralNet_Piano(model_hyper2)\n",
    "#iris_nn.reset_cache()\n",
    "for i in range(50000):\n",
    "    th_batch = i%num_train_batch\n",
    "    Xbatch = trainX[th_batch*batch_size:th_batch*batch_size+batch_size,:]\n",
    "    #print Xbatch\n",
    "    Ybatch = trainY[th_batch*batch_size:th_batch*batch_size+batch_size].astype(int)  \n",
    "    #print Ybatch\n",
    "    loss_iris,dz_iris = iris_nn.forward_train(Xbatch,Ybatch)\n",
    "    if i < num_rec:\n",
    "        cost_rec[i] = loss_iris\n",
    "    print loss_iris\n",
    "    iris_nn.backward_train(dz_iris)\n",
    "    #iris_nn.check_grad_numgrad(Xbatch,Ybatch,0.0001,True)\n",
    "    iris_nn.Apply_UpdateW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cost_rec[0:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
